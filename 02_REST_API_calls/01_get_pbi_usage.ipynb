{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Grab PowerBI usage data data from REST API */admin*\n",
    "\n",
    "- Purpose: \n",
    "    - query timestamp of latest data point from etl table,\n",
    "    - obtain a service pricipal OAUTH token,\n",
    "    - For each day from last point until today: gather transactional JSON data (paginated),\n",
    "    - concat and process data,\n",
    "    - insert data to interface table,\n",
    "    - add loading success/failure and stats to etl table.\n",
    "- Author: vsm\n",
    "- Date: 2025-06-30\n",
    "- Team: GS - BI/ERP\n",
    "\n",
    "### Requirements\n",
    "\n",
    "Python package pyodbc needs a preinstalled ODBC driver on Linux, e.g., Ubuntu `sudo apt install unixodbc` and the\n",
    "actual Microsoft ODBC Driver 17 for SQL Server:\n",
    "```bash\n",
    "# 1. Get underlying packages\n",
    "sudo apt-get update\n",
    "sudo apt-get install -y curl apt-transport-https software-properties-common\n",
    "\n",
    "# 2. Save Microsoft GPG-Key\n",
    "curl -sSL https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor | sudo tee /etc/apt/keyrings/microsoft.gpg > /dev/null\n",
    "\n",
    "# 3. Add repository (for Ubuntu 22.04 „jammy“)\n",
    "echo \"deb [arch=amd64 signed-by=/etc/apt/keyrings/microsoft.gpg] https://packages.microsoft.com/ubuntu/22.04/prod jammy main\" | sudo tee /etc/apt/sources.list.d/mssql-release.list\n",
    "\n",
    "# 4. Reload package list\n",
    "sudo apt-get update\n",
    "\n",
    "# 5. Install\n",
    "sudo ACCEPT_EULA=Y apt-get install -y msodbcsql17 unixodbc-dev\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import hashlib\n",
    "import pyodbc\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# remove orphaned logging handlers\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(asctime)s %(message)s\")\n",
    "\n",
    "env_dict = dotenv_values(\"./.env\")\n",
    "\n",
    "API_CREDS = {\n",
    "    \"TENANT_ID\": env_dict['TENANT_ID'],\n",
    "    \"CLIENT_ID\": env_dict['CLIENT_ID'],\n",
    "    \"CLIENT_SECRET\": env_dict['CLIENT_SECRET'],\n",
    "    \"SCOPE\": env_dict['SCOPE'],\n",
    "    \"OAUTH_URL\": env_dict['OAUTH_URL'].replace('$TENANT_ID', env_dict['TENANT_ID']),\n",
    "    \"REST_URL\": env_dict['REST_URL'],\n",
    "}\n",
    "\n",
    "BI_META = {\n",
    "    \"BI_SERVICE_NAME\": env_dict['BI_SERVICE_NAME'],\n",
    "    \"BI_STAGING_TABLE\": env_dict['BI_STAGING_TABLE'],\n",
    "    \"BI_LOG_TABLE\": env_dict['BI_LOG_TABLE'],\n",
    "    \"BI_INGEST_TS\": env_dict['BI_INGEST_TS'],\n",
    "}\n",
    "\n",
    "SQL_CREDS = {\n",
    "    \"SQL_SERVER\": env_dict['SQL_SERVER'],\n",
    "    \"SQL_PORT\": env_dict['SQL_PORT'],\n",
    "    \"SQL_DB\": env_dict['SQL_DB'],\n",
    "    \"SQL_SCHEMA\": env_dict['SQL_SCHEMA'],\n",
    "    \"SQL_USER\": env_dict['SQL_USER'],\n",
    "    \"SQL_PWD\": env_dict['SQL_PWD'],\n",
    "}\n",
    "\n",
    "logging.info(\"Loading Modules\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SQL Server Connection ===\n",
    "def get_sql_conn():\n",
    "    logging.info(f\"Connecting to DB\")\n",
    "    return pyodbc.connect(\n",
    "        f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "        f\"SERVER={SQL_CREDS['SQL_SERVER']},{SQL_CREDS['SQL_PORT']};\"\n",
    "        f\"PORT={SQL_CREDS['SQL_PORT']};\"        \n",
    "        f\"DATABASE={SQL_CREDS['SQL_DB']};\"\n",
    "        f\"UID={SQL_CREDS['SQL_USER']};\"\n",
    "        f\"PWD={SQL_CREDS['SQL_PWD']}\"\n",
    "    )\n",
    "\n",
    "# === Get latest timestamp ===\n",
    "def get_latest_timestamp(conn):\n",
    "    logging.info(f\"Getting maximum event timestamp\")\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"SELECT COALESCE(MAX(event_time), '2025-06-27') FROM {SQL_CREDS['SQL_SCHEMA']}.{BI_META['BI_STAGING_TABLE']}\")\n",
    "    result = cursor.fetchone()\n",
    "    return result[0] if result else datetime(2025, 6, 27)\n",
    "\n",
    "# === Get Service Principal Token ===\n",
    "def get_access_token():\n",
    "    logging.info(f\"Getting OAuth\")\n",
    "    url = API_CREDS['OAUTH_URL']\n",
    "    data = {\n",
    "        'grant_type': 'client_credentials',\n",
    "        'client_id': API_CREDS['CLIENT_ID'],\n",
    "        'client_secret': API_CREDS['CLIENT_SECRET'],\n",
    "        'scope': API_CREDS['SCOPE']\n",
    "    }\n",
    "    res = requests.post(url, data=data)\n",
    "    res.raise_for_status()\n",
    "    return res.json()['access_token']\n",
    "\n",
    "# === Hash JSON row ===\n",
    "def hash_row(row):\n",
    "    return hashlib.sha256(json.dumps(row, sort_keys=True).encode()).hexdigest()\n",
    "\n",
    "# === Fetch paginated PowerBI usage data ===\n",
    "def fetch_usage_data(date_str, hour_str, token):\n",
    "    logging.info(f\"Getting REST Data\")\n",
    "    rows  = []\n",
    "    url = f\"{API_CREDS['REST_URL']}?startDateTime='{date_str}T{hour_str}:00:00Z'&endDateTime='{date_str}T23:59:59Z'\"\n",
    "    logging.info(f\"\"\"Getting REST Data for url:\\n\"\"\"\n",
    "                 f\"\"\"{url}\"\"\")\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    counter = 1\n",
    "    while url:\n",
    "        logging.info(f\"Page_{counter:02}\")\n",
    "        resp = requests.get(url, headers=headers)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        rows.extend(data.get('activityEventEntities', []))\n",
    "        url = data.get('continuationUri')\n",
    "        counter += 1\n",
    "\n",
    "    return rows\n",
    "\n",
    "# === Insert new data (with hashdiff) ===\n",
    "def insert_new_data(conn, rows):\n",
    "    logging.info(f\"Inserting relevant data to DB\")\n",
    "    inserted_count = 0\n",
    "    duplicate_count =0\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    max_tf_ts = datetime(1900, 1, 1)\n",
    "\n",
    "    for row in rows:\n",
    "        tf_ts_str = row.get(BI_META['BI_INGEST_TS'])\n",
    "        if not tf_ts_str:\n",
    "            logging.warning(f\"JSON payload does not contain {BI_META['BI_INGEST_TS']} key\")\n",
    "            continue  # skip if missing transaction timestamp\n",
    "\n",
    "        try:\n",
    "            tf_ts = datetime.fromisoformat(tf_ts_str)\n",
    "        except ValueError:\n",
    "            logging.warning(f\"Row:{inserted_count + 1}: Timestamp {BI_META['BI_INGEST_TS']} malformed\")\n",
    "            continue  # skip malformed timestamp\n",
    "\n",
    "        hash_val = hash_row(row)\n",
    "        data_str = json.dumps(row)\n",
    "\n",
    "        # Skip duplicates\n",
    "        cursor.execute(f\"SELECT 1 FROM {SQL_CREDS['SQL_SCHEMA']}.{BI_META['BI_STAGING_TABLE']} WHERE hash = ?\", (hash_val,))\n",
    "        if cursor.fetchone():\n",
    "            duplicate_count += 1\n",
    "            continue\n",
    "\n",
    "        # Insert new record\n",
    "        cursor.execute(\n",
    "            f\"INSERT INTO {SQL_CREDS['SQL_SCHEMA']}.{BI_META['BI_STAGING_TABLE']} (event_time, data_json, hash) VALUES (?, ?, ?)\",\n",
    "            (tf_ts, data_str, hash_val)\n",
    "        )\n",
    "        inserted_count += 1\n",
    "\n",
    "        # Update max TF_TIMESTAMP\n",
    "        if not max_tf_ts or tf_ts > max_tf_ts:\n",
    "            max_tf_ts = tf_ts\n",
    "\n",
    "    conn.commit()\n",
    "    logging.info(f\"{inserted_count} rows inserted to DB, {duplicate_count} duplicates skipped\")\n",
    "    return inserted_count, max_tf_ts\n",
    "\n",
    "# === Log ETL run result ===\n",
    "def log_etl_result(conn, success, inserted_rows, max_ts):\n",
    "    logging.info(f\"Logging ETL run to DB\")\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "        f\"INSERT INTO {SQL_CREDS['SQL_SCHEMA']}.{BI_META['BI_LOG_TABLE']} (run_time, service_name, success, inserted_rows, max_event_time) VALUES (?, ?, ?, ?, ?)\",\n",
    "        (datetime.now(timezone.utc), BI_META['BI_SERVICE_NAME'], success, inserted_rows, max_ts)\n",
    "    )\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Main ETL logic ===\n",
    "def run_etl():\n",
    "    conn = get_sql_conn()\n",
    "\n",
    "    # read maximum timestamp in staging_table\n",
    "    latest_ts = get_latest_timestamp(conn)\n",
    "    today = datetime.now(timezone.utc).date()\n",
    "    hour_str = '00'\n",
    "    logging.info(f\"Latest timestamp is: {latest_ts}\")\n",
    "    \n",
    "    inserted_total = 0\n",
    "    max_data_ts = latest_ts\n",
    "\n",
    "    try:\n",
    "        token = get_access_token()\n",
    "\n",
    "        for day in range(0, (today - latest_ts.date()).days + 1):\n",
    "            \n",
    "            if day == 0:\n",
    "                hour_str = f\"{latest_ts.hour:02d}\"\n",
    "\n",
    "            date_str = (latest_ts.date() + timedelta(days=day)).isoformat()\n",
    "\n",
    "            logging.info(f\"Daily extract for: {date_str}, with offset {hour_str} hours.\")\n",
    "            #daily_rows = []\n",
    "            daily_rows = fetch_usage_data(date_str, hour_str, token)\n",
    "\n",
    "            if daily_rows:\n",
    "                inserted, max_tf_ts = insert_new_data(conn, daily_rows)\n",
    "                inserted_total += inserted\n",
    "                if max_tf_ts and max_tf_ts > max_data_ts:\n",
    "                    max_data_ts = max_tf_ts\n",
    "\n",
    "        \n",
    "        log_etl_result(conn, True, inserted_total, max_data_ts)\n",
    "        logging.info(f\"✅ Success: Inserted {inserted_total} records.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_etl_result(conn, False, inserted_total, max_data_ts)\n",
    "        logging.info(f\"❌ Failure: {e}\")\n",
    "\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_etl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
